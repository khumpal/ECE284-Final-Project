{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning 50 ic-slices out of 64 ic-slices (78.1% pruned)\n",
      "Pruning 50 ic-slices out of 64 ic-slices (78.1% pruned)\n",
      "Pruning 100 ic-slices out of 128 ic-slices (78.1% pruned)\n",
      "Pruning 100 ic-slices out of 128 ic-slices (78.1% pruned)\n",
      "Pruning 200 ic-slices out of 256 ic-slices (78.1% pruned)\n",
      "Pruning 200 ic-slices out of 256 ic-slices (78.1% pruned)\n",
      "Pruning 200 ic-slices out of 256 ic-slices (78.1% pruned)\n",
      "Pruning 399 ic-slices out of 512 ic-slices (77.9% pruned)\n",
      "Pruning 399 ic-slices out of 512 ic-slices (77.9% pruned)\n",
      "Pruning 399 ic-slices out of 512 ic-slices (77.9% pruned)\n",
      "Pruning 399 ic-slices out of 512 ic-slices (77.9% pruned)\n",
      "Pruning 399 ic-slices out of 512 ic-slices (77.9% pruned)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63507/652440560.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(f\"result/VGG16_new_os_iter_prune_0.78/model_best.pth.tar\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "from models import *\n",
    "from models.prune_util import *\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "global best_prec\n",
    "\n",
    "batch_size = 64\n",
    "model_name = f\"VGG16_new_os_iter_prune_0.78_q\"\n",
    "fdir = 'result/' + model_name\n",
    "model = VGG16()\n",
    "os_prune_vgg16(model, 0.78)\n",
    "checkpoint = torch.load(f\"result/VGG16_new_os_iter_prune_0.78/model_best.pth.tar\")\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.cuda()\n",
    "\n",
    "device = torch.device(\"cuda\") \n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "print_freq = 100 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
    "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))      \n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "         \n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "        \n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    adjust_list = [150, 225]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1        \n",
    "\n",
    "def train_model(model, fdir, criterion, optimizer, epochs): \n",
    "    os.makedirs(fdir, exist_ok=True)\n",
    "\n",
    "    best_prec = 0\n",
    "\n",
    "    #model = nn.DataParallel(model).cuda()\n",
    "    model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    #cudnn.benchmark = True\n",
    "            \n",
    "\n",
    "    for epoch in range(0, epochs):\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "        train(trainloader, model, criterion, optimizer, epoch)\n",
    "        \n",
    "        # evaluate on test set\n",
    "        print(\"Validation starts\")\n",
    "        prec = validate(testloader, model, criterion)\n",
    "\n",
    "        # remember best precision and save checkpoint\n",
    "        is_best = prec > best_prec\n",
    "        best_prec = max(prec,best_prec)\n",
    "        print('best acc: {:1f}'.format(best_prec))\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec': best_prec,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, is_best, fdir)\n",
    "\n",
    "def val_model(model): \n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in testloader:\n",
    "            data, target = data.to(device), target.to(device) # loading to GPU\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)  \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(testloader.dataset)\n",
    "\n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            correct, len(testloader.dataset),\n",
    "            100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 8668/10000 (87%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 1003/10000 (10%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quantize_pruned(model)\n",
    "\n",
    "val_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/782]\tTime 0.173 (0.173)\tData 0.020 (0.020)\tLoss 1.4387 (1.4387)\tPrec 56.250% (56.250%)\n",
      "Epoch: [0][100/782]\tTime 0.067 (0.068)\tData 0.012 (0.016)\tLoss 0.7347 (0.9887)\tPrec 73.438% (67.141%)\n",
      "Epoch: [0][200/782]\tTime 0.067 (0.068)\tData 0.014 (0.017)\tLoss 0.6829 (0.8627)\tPrec 81.250% (71.152%)\n",
      "Epoch: [0][300/782]\tTime 0.058 (0.067)\tData 0.023 (0.017)\tLoss 0.7700 (0.7957)\tPrec 71.875% (73.406%)\n",
      "Epoch: [0][400/782]\tTime 0.066 (0.067)\tData 0.013 (0.017)\tLoss 0.7631 (0.7527)\tPrec 65.625% (74.716%)\n",
      "Epoch: [0][500/782]\tTime 0.067 (0.067)\tData 0.013 (0.016)\tLoss 0.6015 (0.7195)\tPrec 78.125% (75.646%)\n",
      "Epoch: [0][600/782]\tTime 0.067 (0.067)\tData 0.013 (0.016)\tLoss 0.5384 (0.6885)\tPrec 79.688% (76.630%)\n",
      "Epoch: [0][700/782]\tTime 0.066 (0.067)\tData 0.012 (0.016)\tLoss 0.6657 (0.6642)\tPrec 81.250% (77.441%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.033 (0.033)\tLoss 0.6285 (0.6285)\tPrec 81.250% (81.250%)\n",
      "Test: [100/157]\tTime 0.033 (0.033)\tLoss 0.6762 (0.5937)\tPrec 79.688% (80.554%)\n",
      " * Prec 80.700% \n",
      "best acc: 80.700000\n",
      "Epoch: [1][0/782]\tTime 0.055 (0.055)\tData 0.020 (0.020)\tLoss 0.5163 (0.5163)\tPrec 81.250% (81.250%)\n",
      "Epoch: [1][100/782]\tTime 0.066 (0.067)\tData 0.012 (0.013)\tLoss 0.3526 (0.5094)\tPrec 87.500% (82.457%)\n",
      "Epoch: [1][200/782]\tTime 0.067 (0.067)\tData 0.017 (0.014)\tLoss 0.2990 (0.5078)\tPrec 90.625% (82.478%)\n",
      "Epoch: [1][300/782]\tTime 0.067 (0.067)\tData 0.012 (0.015)\tLoss 0.5374 (0.5051)\tPrec 79.688% (82.729%)\n",
      "Epoch: [1][400/782]\tTime 0.068 (0.067)\tData 0.013 (0.015)\tLoss 0.3316 (0.5044)\tPrec 87.500% (82.746%)\n",
      "Epoch: [1][500/782]\tTime 0.066 (0.067)\tData 0.012 (0.015)\tLoss 0.5308 (0.5028)\tPrec 82.812% (82.850%)\n",
      "Epoch: [1][600/782]\tTime 0.066 (0.067)\tData 0.012 (0.015)\tLoss 0.4872 (0.4976)\tPrec 81.250% (83.049%)\n",
      "Epoch: [1][700/782]\tTime 0.067 (0.067)\tData 0.012 (0.015)\tLoss 0.2464 (0.4921)\tPrec 93.750% (83.214%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.032 (0.032)\tLoss 0.6379 (0.6379)\tPrec 76.562% (76.562%)\n",
      "Test: [100/157]\tTime 0.031 (0.032)\tLoss 0.6368 (0.5566)\tPrec 78.125% (81.683%)\n",
      " * Prec 81.380% \n",
      "best acc: 81.380000\n",
      "Epoch: [2][0/782]\tTime 0.054 (0.054)\tData 0.017 (0.017)\tLoss 0.4469 (0.4469)\tPrec 85.938% (85.938%)\n",
      "Epoch: [2][100/782]\tTime 0.067 (0.067)\tData 0.012 (0.015)\tLoss 0.5799 (0.4458)\tPrec 81.250% (84.437%)\n",
      "Epoch: [2][200/782]\tTime 0.066 (0.067)\tData 0.012 (0.015)\tLoss 0.2558 (0.4451)\tPrec 87.500% (84.569%)\n",
      "Epoch: [2][300/782]\tTime 0.081 (0.067)\tData 0.025 (0.016)\tLoss 0.5199 (0.4441)\tPrec 81.250% (84.697%)\n",
      "Epoch: [2][400/782]\tTime 0.067 (0.067)\tData 0.012 (0.016)\tLoss 0.3393 (0.4410)\tPrec 82.812% (84.749%)\n",
      "Epoch: [2][500/782]\tTime 0.067 (0.067)\tData 0.025 (0.016)\tLoss 0.4661 (0.4401)\tPrec 82.812% (84.740%)\n",
      "Epoch: [2][600/782]\tTime 0.066 (0.067)\tData 0.012 (0.016)\tLoss 0.4827 (0.4415)\tPrec 84.375% (84.726%)\n",
      "Epoch: [2][700/782]\tTime 0.066 (0.067)\tData 0.014 (0.015)\tLoss 0.4847 (0.4399)\tPrec 87.500% (84.692%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.042 (0.042)\tLoss 0.6080 (0.6080)\tPrec 78.125% (78.125%)\n",
      "Test: [100/157]\tTime 0.031 (0.031)\tLoss 0.5666 (0.5259)\tPrec 82.812% (82.317%)\n",
      " * Prec 82.520% \n",
      "best acc: 82.520000\n",
      "Epoch: [3][0/782]\tTime 0.067 (0.067)\tData 0.021 (0.021)\tLoss 0.4982 (0.4982)\tPrec 78.125% (78.125%)\n",
      "Epoch: [3][100/782]\tTime 0.067 (0.067)\tData 0.012 (0.014)\tLoss 0.4800 (0.4166)\tPrec 78.125% (85.319%)\n",
      "Epoch: [3][200/782]\tTime 0.067 (0.067)\tData 0.012 (0.015)\tLoss 0.3631 (0.4093)\tPrec 84.375% (85.634%)\n",
      "Epoch: [3][300/782]\tTime 0.066 (0.067)\tData 0.025 (0.015)\tLoss 0.3002 (0.4097)\tPrec 90.625% (85.803%)\n",
      "Epoch: [3][400/782]\tTime 0.066 (0.067)\tData 0.012 (0.015)\tLoss 0.4937 (0.4099)\tPrec 84.375% (85.887%)\n",
      "Epoch: [3][500/782]\tTime 0.069 (0.067)\tData 0.012 (0.015)\tLoss 0.3281 (0.4134)\tPrec 87.500% (85.841%)\n",
      "Epoch: [3][600/782]\tTime 0.067 (0.067)\tData 0.012 (0.015)\tLoss 0.2723 (0.4109)\tPrec 92.188% (85.940%)\n",
      "Epoch: [3][700/782]\tTime 0.067 (0.067)\tData 0.017 (0.015)\tLoss 0.4651 (0.4096)\tPrec 87.500% (86.009%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.041 (0.041)\tLoss 0.6682 (0.6682)\tPrec 76.562% (76.562%)\n",
      "Test: [100/157]\tTime 0.031 (0.031)\tLoss 0.5772 (0.4791)\tPrec 84.375% (84.421%)\n",
      " * Prec 84.230% \n",
      "best acc: 84.230000\n",
      "Epoch: [4][0/782]\tTime 0.068 (0.068)\tData 0.020 (0.020)\tLoss 0.2113 (0.2113)\tPrec 93.750% (93.750%)\n",
      "Epoch: [4][100/782]\tTime 0.066 (0.067)\tData 0.025 (0.016)\tLoss 0.4706 (0.3761)\tPrec 85.938% (86.989%)\n",
      "Epoch: [4][200/782]\tTime 0.066 (0.067)\tData 0.025 (0.016)\tLoss 0.3334 (0.3877)\tPrec 92.188% (86.707%)\n",
      "Epoch: [4][300/782]\tTime 0.066 (0.067)\tData 0.012 (0.016)\tLoss 0.6545 (0.3896)\tPrec 75.000% (86.534%)\n",
      "Epoch: [4][400/782]\tTime 0.079 (0.067)\tData 0.013 (0.016)\tLoss 0.4184 (0.3925)\tPrec 85.938% (86.366%)\n",
      "Epoch: [4][500/782]\tTime 0.080 (0.067)\tData 0.013 (0.016)\tLoss 0.4509 (0.3892)\tPrec 89.062% (86.524%)\n",
      "Epoch: [4][600/782]\tTime 0.067 (0.067)\tData 0.023 (0.016)\tLoss 0.4059 (0.3869)\tPrec 84.375% (86.556%)\n",
      "Epoch: [4][700/782]\tTime 0.059 (0.067)\tData 0.012 (0.016)\tLoss 0.3918 (0.3874)\tPrec 84.375% (86.577%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.042 (0.042)\tLoss 0.7254 (0.7254)\tPrec 81.250% (81.250%)\n",
      "Test: [100/157]\tTime 0.032 (0.032)\tLoss 0.4934 (0.5331)\tPrec 84.375% (82.936%)\n",
      " * Prec 83.410% \n",
      "best acc: 84.230000\n",
      "Epoch: [5][0/782]\tTime 0.063 (0.063)\tData 0.018 (0.018)\tLoss 0.5463 (0.5463)\tPrec 84.375% (84.375%)\n",
      "Epoch: [5][100/782]\tTime 0.080 (0.069)\tData 0.020 (0.016)\tLoss 0.3303 (0.3789)\tPrec 87.500% (87.113%)\n",
      "Epoch: [5][200/782]\tTime 0.076 (0.070)\tData 0.012 (0.016)\tLoss 0.3625 (0.3774)\tPrec 85.938% (87.306%)\n",
      "Epoch: [5][300/782]\tTime 0.069 (0.069)\tData 0.014 (0.016)\tLoss 0.2467 (0.3749)\tPrec 92.188% (87.230%)\n",
      "Epoch: [5][400/782]\tTime 0.067 (0.070)\tData 0.014 (0.016)\tLoss 0.2644 (0.3734)\tPrec 92.188% (87.204%)\n",
      "Epoch: [5][500/782]\tTime 0.066 (0.070)\tData 0.014 (0.017)\tLoss 0.2668 (0.3724)\tPrec 90.625% (87.229%)\n",
      "Epoch: [5][600/782]\tTime 0.067 (0.070)\tData 0.018 (0.017)\tLoss 0.3488 (0.3738)\tPrec 89.062% (87.162%)\n",
      "Epoch: [5][700/782]\tTime 0.068 (0.070)\tData 0.013 (0.016)\tLoss 0.4182 (0.3713)\tPrec 82.812% (87.293%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.036 (0.036)\tLoss 0.5558 (0.5558)\tPrec 82.812% (82.812%)\n",
      "Test: [100/157]\tTime 0.032 (0.033)\tLoss 0.5938 (0.5169)\tPrec 78.125% (83.014%)\n",
      " * Prec 83.530% \n",
      "best acc: 84.230000\n",
      "Epoch: [6][0/782]\tTime 0.075 (0.075)\tData 0.019 (0.019)\tLoss 0.3723 (0.3723)\tPrec 82.812% (82.812%)\n",
      "Epoch: [6][100/782]\tTime 0.066 (0.069)\tData 0.012 (0.017)\tLoss 0.3776 (0.3500)\tPrec 85.938% (87.701%)\n",
      "Epoch: [6][200/782]\tTime 0.066 (0.069)\tData 0.012 (0.017)\tLoss 0.2301 (0.3501)\tPrec 92.188% (87.803%)\n",
      "Epoch: [6][300/782]\tTime 0.065 (0.069)\tData 0.012 (0.016)\tLoss 0.3576 (0.3555)\tPrec 87.500% (87.443%)\n",
      "Epoch: [6][400/782]\tTime 0.068 (0.069)\tData 0.012 (0.016)\tLoss 0.3940 (0.3597)\tPrec 89.062% (87.329%)\n",
      "Epoch: [6][500/782]\tTime 0.066 (0.069)\tData 0.012 (0.016)\tLoss 0.4635 (0.3596)\tPrec 82.812% (87.363%)\n",
      "Epoch: [6][600/782]\tTime 0.080 (0.069)\tData 0.012 (0.016)\tLoss 0.4442 (0.3566)\tPrec 85.938% (87.458%)\n",
      "Epoch: [6][700/782]\tTime 0.072 (0.069)\tData 0.013 (0.016)\tLoss 0.3136 (0.3573)\tPrec 85.938% (87.418%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.040 (0.040)\tLoss 0.5201 (0.5201)\tPrec 82.812% (82.812%)\n",
      "Test: [100/157]\tTime 0.036 (0.033)\tLoss 0.5104 (0.4709)\tPrec 82.812% (83.942%)\n",
      " * Prec 84.160% \n",
      "best acc: 84.230000\n",
      "Epoch: [7][0/782]\tTime 0.052 (0.052)\tData 0.017 (0.017)\tLoss 0.2722 (0.2722)\tPrec 89.062% (89.062%)\n",
      "Epoch: [7][100/782]\tTime 0.065 (0.069)\tData 0.012 (0.012)\tLoss 0.3669 (0.3379)\tPrec 87.500% (88.181%)\n",
      "Epoch: [7][200/782]\tTime 0.067 (0.069)\tData 0.012 (0.013)\tLoss 0.2528 (0.3446)\tPrec 92.188% (87.966%)\n",
      "Epoch: [7][300/782]\tTime 0.073 (0.069)\tData 0.012 (0.013)\tLoss 0.3744 (0.3413)\tPrec 85.938% (87.998%)\n",
      "Epoch: [7][400/782]\tTime 0.070 (0.069)\tData 0.012 (0.013)\tLoss 0.2796 (0.3394)\tPrec 89.062% (88.010%)\n",
      "Epoch: [7][500/782]\tTime 0.082 (0.069)\tData 0.012 (0.013)\tLoss 0.3060 (0.3391)\tPrec 84.375% (88.043%)\n",
      "Epoch: [7][600/782]\tTime 0.067 (0.069)\tData 0.013 (0.013)\tLoss 0.4865 (0.3456)\tPrec 85.938% (87.887%)\n",
      "Epoch: [7][700/782]\tTime 0.066 (0.069)\tData 0.012 (0.012)\tLoss 0.3364 (0.3471)\tPrec 90.625% (87.814%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.044 (0.044)\tLoss 0.5684 (0.5684)\tPrec 82.812% (82.812%)\n",
      "Test: [100/157]\tTime 0.032 (0.032)\tLoss 0.6045 (0.4575)\tPrec 82.812% (85.675%)\n",
      " * Prec 85.520% \n",
      "best acc: 85.520000\n",
      "Epoch: [8][0/782]\tTime 0.053 (0.053)\tData 0.018 (0.018)\tLoss 0.3203 (0.3203)\tPrec 89.062% (89.062%)\n",
      "Epoch: [8][100/782]\tTime 0.078 (0.067)\tData 0.012 (0.013)\tLoss 0.1750 (0.3321)\tPrec 95.312% (88.846%)\n",
      "Epoch: [8][200/782]\tTime 0.067 (0.067)\tData 0.012 (0.013)\tLoss 0.2982 (0.3321)\tPrec 87.500% (88.433%)\n",
      "Epoch: [8][300/782]\tTime 0.067 (0.067)\tData 0.012 (0.013)\tLoss 0.3502 (0.3343)\tPrec 85.938% (88.367%)\n",
      "Epoch: [8][400/782]\tTime 0.082 (0.067)\tData 0.012 (0.013)\tLoss 0.2993 (0.3358)\tPrec 93.750% (88.221%)\n",
      "Epoch: [8][500/782]\tTime 0.067 (0.067)\tData 0.012 (0.012)\tLoss 0.4060 (0.3402)\tPrec 85.938% (88.061%)\n",
      "Epoch: [8][600/782]\tTime 0.068 (0.067)\tData 0.012 (0.012)\tLoss 0.3750 (0.3392)\tPrec 87.500% (88.127%)\n",
      "Epoch: [8][700/782]\tTime 0.067 (0.067)\tData 0.012 (0.012)\tLoss 0.3071 (0.3409)\tPrec 89.062% (88.109%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.043 (0.043)\tLoss 0.5354 (0.5354)\tPrec 84.375% (84.375%)\n",
      "Test: [100/157]\tTime 0.031 (0.032)\tLoss 0.5039 (0.4467)\tPrec 85.938% (85.458%)\n",
      " * Prec 85.320% \n",
      "best acc: 85.520000\n",
      "Epoch: [9][0/782]\tTime 0.048 (0.048)\tData 0.014 (0.014)\tLoss 0.2906 (0.2906)\tPrec 84.375% (84.375%)\n",
      "Epoch: [9][100/782]\tTime 0.067 (0.067)\tData 0.013 (0.013)\tLoss 0.4879 (0.3340)\tPrec 87.500% (88.444%)\n",
      "Epoch: [9][200/782]\tTime 0.066 (0.067)\tData 0.012 (0.013)\tLoss 0.2935 (0.3378)\tPrec 90.625% (88.262%)\n",
      "Epoch: [9][300/782]\tTime 0.067 (0.067)\tData 0.012 (0.013)\tLoss 0.2814 (0.3366)\tPrec 89.062% (88.273%)\n",
      "Epoch: [9][400/782]\tTime 0.067 (0.067)\tData 0.013 (0.013)\tLoss 0.2189 (0.3321)\tPrec 93.750% (88.303%)\n",
      "Epoch: [9][500/782]\tTime 0.056 (0.067)\tData 0.012 (0.013)\tLoss 0.2481 (0.3316)\tPrec 92.188% (88.426%)\n",
      "Epoch: [9][600/782]\tTime 0.067 (0.067)\tData 0.012 (0.013)\tLoss 0.4197 (0.3352)\tPrec 87.500% (88.353%)\n",
      "Epoch: [9][700/782]\tTime 0.067 (0.067)\tData 0.012 (0.013)\tLoss 0.2051 (0.3338)\tPrec 92.188% (88.403%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.036 (0.036)\tLoss 0.4923 (0.4923)\tPrec 82.812% (82.812%)\n",
      "Test: [100/157]\tTime 0.031 (0.031)\tLoss 0.6052 (0.4344)\tPrec 79.688% (85.907%)\n",
      " * Prec 85.900% \n",
      "best acc: 85.900000\n",
      "Epoch: [10][0/782]\tTime 0.051 (0.051)\tData 0.018 (0.018)\tLoss 0.4092 (0.4092)\tPrec 87.500% (87.500%)\n",
      "Epoch: [10][100/782]\tTime 0.059 (0.067)\tData 0.013 (0.015)\tLoss 0.2738 (0.3129)\tPrec 90.625% (89.016%)\n",
      "Epoch: [10][200/782]\tTime 0.067 (0.067)\tData 0.024 (0.014)\tLoss 0.2670 (0.3146)\tPrec 90.625% (88.822%)\n",
      "Epoch: [10][300/782]\tTime 0.081 (0.067)\tData 0.012 (0.014)\tLoss 0.4313 (0.3151)\tPrec 87.500% (89.052%)\n",
      "Epoch: [10][400/782]\tTime 0.066 (0.067)\tData 0.013 (0.014)\tLoss 0.4553 (0.3182)\tPrec 85.938% (88.988%)\n",
      "Epoch: [10][500/782]\tTime 0.076 (0.067)\tData 0.012 (0.014)\tLoss 0.4165 (0.3217)\tPrec 84.375% (88.863%)\n",
      "Epoch: [10][600/782]\tTime 0.081 (0.067)\tData 0.012 (0.014)\tLoss 0.2723 (0.3241)\tPrec 90.625% (88.738%)\n",
      "Epoch: [10][700/782]\tTime 0.066 (0.067)\tData 0.012 (0.014)\tLoss 0.2164 (0.3229)\tPrec 90.625% (88.802%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.041 (0.041)\tLoss 0.6895 (0.6895)\tPrec 78.125% (78.125%)\n",
      "Test: [100/157]\tTime 0.031 (0.032)\tLoss 0.6020 (0.4593)\tPrec 81.250% (85.118%)\n",
      " * Prec 84.960% \n",
      "best acc: 85.900000\n",
      "Epoch: [11][0/782]\tTime 0.055 (0.055)\tData 0.021 (0.021)\tLoss 0.4376 (0.4376)\tPrec 84.375% (84.375%)\n",
      "Epoch: [11][100/782]\tTime 0.067 (0.067)\tData 0.012 (0.014)\tLoss 0.2769 (0.3081)\tPrec 90.625% (89.403%)\n",
      "Epoch: [11][200/782]\tTime 0.056 (0.067)\tData 0.021 (0.014)\tLoss 0.4523 (0.3143)\tPrec 87.500% (89.055%)\n",
      "Epoch: [11][300/782]\tTime 0.067 (0.067)\tData 0.013 (0.015)\tLoss 0.3058 (0.3115)\tPrec 90.625% (89.120%)\n",
      "Epoch: [11][400/782]\tTime 0.081 (0.067)\tData 0.025 (0.015)\tLoss 0.1639 (0.3145)\tPrec 93.750% (89.062%)\n",
      "Epoch: [11][500/782]\tTime 0.067 (0.067)\tData 0.013 (0.015)\tLoss 0.3476 (0.3127)\tPrec 87.500% (89.134%)\n",
      "Epoch: [11][600/782]\tTime 0.067 (0.067)\tData 0.025 (0.015)\tLoss 0.2169 (0.3139)\tPrec 93.750% (89.096%)\n",
      "Epoch: [11][700/782]\tTime 0.066 (0.067)\tData 0.012 (0.015)\tLoss 0.3339 (0.3164)\tPrec 84.375% (88.991%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.042 (0.042)\tLoss 0.5320 (0.5320)\tPrec 82.812% (82.812%)\n",
      "Test: [100/157]\tTime 0.031 (0.032)\tLoss 0.5328 (0.4409)\tPrec 79.688% (85.582%)\n",
      " * Prec 85.550% \n",
      "best acc: 85.900000\n",
      "Epoch: [12][0/782]\tTime 0.064 (0.064)\tData 0.015 (0.015)\tLoss 0.2792 (0.2792)\tPrec 92.188% (92.188%)\n",
      "Epoch: [12][100/782]\tTime 0.061 (0.068)\tData 0.014 (0.013)\tLoss 0.3029 (0.3093)\tPrec 89.062% (89.449%)\n",
      "Epoch: [12][200/782]\tTime 0.071 (0.069)\tData 0.013 (0.013)\tLoss 0.3185 (0.3095)\tPrec 84.375% (89.405%)\n",
      "Epoch: [12][300/782]\tTime 0.066 (0.069)\tData 0.012 (0.013)\tLoss 0.2357 (0.3083)\tPrec 93.750% (89.348%)\n",
      "Epoch: [12][400/782]\tTime 0.066 (0.068)\tData 0.013 (0.013)\tLoss 0.2761 (0.3074)\tPrec 87.500% (89.347%)\n",
      "Epoch: [12][500/782]\tTime 0.066 (0.068)\tData 0.013 (0.013)\tLoss 0.3031 (0.3112)\tPrec 89.062% (89.250%)\n",
      "Epoch: [12][600/782]\tTime 0.066 (0.067)\tData 0.012 (0.013)\tLoss 0.2406 (0.3087)\tPrec 85.938% (89.346%)\n",
      "Epoch: [12][700/782]\tTime 0.049 (0.067)\tData 0.012 (0.013)\tLoss 0.2793 (0.3112)\tPrec 90.625% (89.259%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.031 (0.031)\tLoss 0.4463 (0.4463)\tPrec 85.938% (85.938%)\n",
      "Test: [100/157]\tTime 0.031 (0.031)\tLoss 0.4596 (0.4409)\tPrec 84.375% (85.288%)\n",
      " * Prec 85.440% \n",
      "best acc: 85.900000\n",
      "Epoch: [13][0/782]\tTime 0.056 (0.056)\tData 0.018 (0.018)\tLoss 0.2230 (0.2230)\tPrec 92.188% (92.188%)\n",
      "Epoch: [13][100/782]\tTime 0.066 (0.066)\tData 0.012 (0.013)\tLoss 0.3019 (0.2986)\tPrec 90.625% (89.480%)\n",
      "Epoch: [13][200/782]\tTime 0.066 (0.066)\tData 0.012 (0.012)\tLoss 0.2039 (0.2987)\tPrec 90.625% (89.560%)\n",
      "Epoch: [13][300/782]\tTime 0.066 (0.066)\tData 0.012 (0.012)\tLoss 0.4097 (0.2987)\tPrec 85.938% (89.540%)\n",
      "Epoch: [13][400/782]\tTime 0.055 (0.066)\tData 0.012 (0.012)\tLoss 0.3254 (0.3022)\tPrec 90.625% (89.491%)\n",
      "Epoch: [13][500/782]\tTime 0.067 (0.066)\tData 0.012 (0.012)\tLoss 0.2570 (0.3011)\tPrec 87.500% (89.518%)\n",
      "Epoch: [13][600/782]\tTime 0.051 (0.066)\tData 0.012 (0.012)\tLoss 0.4356 (0.2985)\tPrec 82.812% (89.556%)\n",
      "Epoch: [13][700/782]\tTime 0.073 (0.067)\tData 0.012 (0.013)\tLoss 0.3405 (0.3017)\tPrec 85.938% (89.395%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.044 (0.044)\tLoss 0.4922 (0.4922)\tPrec 85.938% (85.938%)\n",
      "Test: [100/157]\tTime 0.032 (0.033)\tLoss 0.6277 (0.4317)\tPrec 84.375% (86.077%)\n",
      " * Prec 86.190% \n",
      "best acc: 86.190000\n",
      "Epoch: [14][0/782]\tTime 0.063 (0.063)\tData 0.018 (0.018)\tLoss 0.2245 (0.2245)\tPrec 96.875% (96.875%)\n",
      "Epoch: [14][100/782]\tTime 0.094 (0.070)\tData 0.026 (0.014)\tLoss 0.5881 (0.2888)\tPrec 89.062% (90.362%)\n",
      "Epoch: [14][200/782]\tTime 0.067 (0.069)\tData 0.016 (0.014)\tLoss 0.2402 (0.2944)\tPrec 92.188% (89.941%)\n",
      "Epoch: [14][300/782]\tTime 0.071 (0.070)\tData 0.013 (0.013)\tLoss 0.1407 (0.2989)\tPrec 95.312% (89.691%)\n",
      "Epoch: [14][400/782]\tTime 0.079 (0.070)\tData 0.012 (0.013)\tLoss 0.2708 (0.3028)\tPrec 85.938% (89.499%)\n",
      "Epoch: [14][500/782]\tTime 0.067 (0.070)\tData 0.013 (0.014)\tLoss 0.3864 (0.3032)\tPrec 84.375% (89.496%)\n",
      "Epoch: [14][600/782]\tTime 0.070 (0.070)\tData 0.013 (0.014)\tLoss 0.3263 (0.3005)\tPrec 84.375% (89.559%)\n",
      "Epoch: [14][700/782]\tTime 0.067 (0.069)\tData 0.024 (0.014)\tLoss 0.3595 (0.3017)\tPrec 85.938% (89.522%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.040 (0.040)\tLoss 0.4792 (0.4792)\tPrec 85.938% (85.938%)\n",
      "Test: [100/157]\tTime 0.032 (0.032)\tLoss 0.6689 (0.4263)\tPrec 79.688% (86.231%)\n",
      " * Prec 86.260% \n",
      "best acc: 86.260000\n",
      "Epoch: [15][0/782]\tTime 0.053 (0.053)\tData 0.019 (0.019)\tLoss 0.3615 (0.3615)\tPrec 85.938% (85.938%)\n",
      "Epoch: [15][100/782]\tTime 0.070 (0.069)\tData 0.013 (0.013)\tLoss 0.2011 (0.3015)\tPrec 93.750% (89.295%)\n",
      "Epoch: [15][200/782]\tTime 0.071 (0.069)\tData 0.012 (0.013)\tLoss 0.1681 (0.2926)\tPrec 95.312% (89.731%)\n",
      "Epoch: [15][300/782]\tTime 0.068 (0.069)\tData 0.012 (0.013)\tLoss 0.4032 (0.2951)\tPrec 84.375% (89.675%)\n",
      "Epoch: [15][400/782]\tTime 0.069 (0.069)\tData 0.012 (0.013)\tLoss 0.2882 (0.2929)\tPrec 89.062% (89.717%)\n",
      "Epoch: [15][500/782]\tTime 0.066 (0.069)\tData 0.012 (0.013)\tLoss 0.1945 (0.2945)\tPrec 95.312% (89.643%)\n",
      "Epoch: [15][600/782]\tTime 0.066 (0.069)\tData 0.013 (0.013)\tLoss 0.2649 (0.2967)\tPrec 89.062% (89.707%)\n",
      "Epoch: [15][700/782]\tTime 0.082 (0.068)\tData 0.012 (0.013)\tLoss 0.3953 (0.2984)\tPrec 85.938% (89.651%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.037 (0.037)\tLoss 0.4754 (0.4754)\tPrec 87.500% (87.500%)\n",
      "Test: [100/157]\tTime 0.031 (0.031)\tLoss 0.5311 (0.4543)\tPrec 82.812% (85.473%)\n",
      " * Prec 85.490% \n",
      "best acc: 86.260000\n",
      "Epoch: [16][0/782]\tTime 0.072 (0.072)\tData 0.019 (0.019)\tLoss 0.3101 (0.3101)\tPrec 87.500% (87.500%)\n",
      "Epoch: [16][100/782]\tTime 0.065 (0.067)\tData 0.024 (0.013)\tLoss 0.2178 (0.2795)\tPrec 95.312% (90.455%)\n",
      "Epoch: [16][200/782]\tTime 0.067 (0.067)\tData 0.012 (0.014)\tLoss 0.3059 (0.2810)\tPrec 87.500% (90.275%)\n",
      "Epoch: [16][300/782]\tTime 0.065 (0.067)\tData 0.013 (0.013)\tLoss 0.4226 (0.2852)\tPrec 85.938% (90.049%)\n",
      "Epoch: [16][400/782]\tTime 0.056 (0.067)\tData 0.012 (0.013)\tLoss 0.1969 (0.2862)\tPrec 93.750% (89.994%)\n",
      "Epoch: [16][500/782]\tTime 0.066 (0.067)\tData 0.012 (0.013)\tLoss 0.2077 (0.2890)\tPrec 93.750% (89.770%)\n",
      "Epoch: [16][600/782]\tTime 0.089 (0.067)\tData 0.013 (0.013)\tLoss 0.2531 (0.2911)\tPrec 90.625% (89.751%)\n",
      "Epoch: [16][700/782]\tTime 0.078 (0.067)\tData 0.012 (0.013)\tLoss 0.3320 (0.2897)\tPrec 89.062% (89.727%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.045 (0.045)\tLoss 0.4519 (0.4519)\tPrec 82.812% (82.812%)\n",
      "Test: [100/157]\tTime 0.031 (0.031)\tLoss 0.7962 (0.4526)\tPrec 81.250% (85.752%)\n",
      " * Prec 85.980% \n",
      "best acc: 86.260000\n",
      "Epoch: [17][0/782]\tTime 0.051 (0.051)\tData 0.018 (0.018)\tLoss 0.4575 (0.4575)\tPrec 81.250% (81.250%)\n",
      "Epoch: [17][100/782]\tTime 0.057 (0.067)\tData 0.012 (0.014)\tLoss 0.4422 (0.2696)\tPrec 84.375% (90.285%)\n",
      "Epoch: [17][200/782]\tTime 0.067 (0.067)\tData 0.014 (0.014)\tLoss 0.2946 (0.2926)\tPrec 87.500% (89.544%)\n",
      "Epoch: [17][300/782]\tTime 0.065 (0.067)\tData 0.012 (0.014)\tLoss 0.3099 (0.2916)\tPrec 89.062% (89.665%)\n",
      "Epoch: [17][400/782]\tTime 0.046 (0.067)\tData 0.012 (0.013)\tLoss 0.2977 (0.2896)\tPrec 90.625% (89.678%)\n",
      "Epoch: [17][500/782]\tTime 0.055 (0.067)\tData 0.012 (0.013)\tLoss 0.2485 (0.2915)\tPrec 87.500% (89.655%)\n",
      "Epoch: [17][600/782]\tTime 0.068 (0.067)\tData 0.012 (0.013)\tLoss 0.1458 (0.2909)\tPrec 95.312% (89.666%)\n",
      "Epoch: [17][700/782]\tTime 0.079 (0.068)\tData 0.013 (0.013)\tLoss 0.1896 (0.2894)\tPrec 93.750% (89.787%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.048 (0.048)\tLoss 0.5368 (0.5368)\tPrec 82.812% (82.812%)\n",
      "Test: [100/157]\tTime 0.031 (0.032)\tLoss 0.6371 (0.4314)\tPrec 78.125% (86.170%)\n",
      " * Prec 86.280% \n",
      "best acc: 86.280000\n",
      "Epoch: [18][0/782]\tTime 0.060 (0.060)\tData 0.021 (0.021)\tLoss 0.2070 (0.2070)\tPrec 90.625% (90.625%)\n",
      "Epoch: [18][100/782]\tTime 0.066 (0.067)\tData 0.026 (0.014)\tLoss 0.2068 (0.2741)\tPrec 90.625% (90.377%)\n",
      "Epoch: [18][200/782]\tTime 0.065 (0.067)\tData 0.013 (0.014)\tLoss 0.1611 (0.2694)\tPrec 92.188% (90.516%)\n",
      "Epoch: [18][300/782]\tTime 0.055 (0.067)\tData 0.016 (0.014)\tLoss 0.2864 (0.2799)\tPrec 87.500% (90.194%)\n",
      "Epoch: [18][400/782]\tTime 0.068 (0.067)\tData 0.014 (0.014)\tLoss 0.2174 (0.2837)\tPrec 90.625% (90.041%)\n",
      "Epoch: [18][500/782]\tTime 0.066 (0.067)\tData 0.012 (0.015)\tLoss 0.6793 (0.2803)\tPrec 81.250% (90.204%)\n",
      "Epoch: [18][600/782]\tTime 0.079 (0.067)\tData 0.027 (0.015)\tLoss 0.1647 (0.2788)\tPrec 96.875% (90.245%)\n",
      "Epoch: [18][700/782]\tTime 0.066 (0.067)\tData 0.017 (0.015)\tLoss 0.2261 (0.2780)\tPrec 92.188% (90.268%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.042 (0.042)\tLoss 0.4978 (0.4978)\tPrec 89.062% (89.062%)\n",
      "Test: [100/157]\tTime 0.032 (0.032)\tLoss 0.5231 (0.4478)\tPrec 78.125% (85.257%)\n",
      " * Prec 85.100% \n",
      "best acc: 86.280000\n",
      "Epoch: [19][0/782]\tTime 0.057 (0.057)\tData 0.021 (0.021)\tLoss 0.2057 (0.2057)\tPrec 92.188% (92.188%)\n",
      "Epoch: [19][100/782]\tTime 0.047 (0.067)\tData 0.012 (0.013)\tLoss 0.3975 (0.2805)\tPrec 85.938% (90.300%)\n",
      "Epoch: [19][200/782]\tTime 0.054 (0.067)\tData 0.012 (0.013)\tLoss 0.3301 (0.2773)\tPrec 89.062% (90.299%)\n",
      "Epoch: [19][300/782]\tTime 0.067 (0.067)\tData 0.012 (0.013)\tLoss 0.1158 (0.2796)\tPrec 96.875% (90.163%)\n",
      "Epoch: [19][400/782]\tTime 0.068 (0.067)\tData 0.012 (0.013)\tLoss 0.2609 (0.2779)\tPrec 92.188% (90.224%)\n",
      "Epoch: [19][500/782]\tTime 0.066 (0.067)\tData 0.012 (0.013)\tLoss 0.3735 (0.2771)\tPrec 84.375% (90.226%)\n",
      "Epoch: [19][600/782]\tTime 0.088 (0.067)\tData 0.012 (0.013)\tLoss 0.3018 (0.2787)\tPrec 90.625% (90.206%)\n",
      "Epoch: [19][700/782]\tTime 0.067 (0.067)\tData 0.012 (0.013)\tLoss 0.3582 (0.2784)\tPrec 85.938% (90.204%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.050 (0.050)\tLoss 0.5622 (0.5622)\tPrec 85.938% (85.938%)\n",
      "Test: [100/157]\tTime 0.031 (0.031)\tLoss 0.4428 (0.4464)\tPrec 84.375% (85.814%)\n",
      " * Prec 85.920% \n",
      "best acc: 86.280000\n",
      "Epoch: [20][0/782]\tTime 0.065 (0.065)\tData 0.017 (0.017)\tLoss 0.2226 (0.2226)\tPrec 93.750% (93.750%)\n",
      "Epoch: [20][100/782]\tTime 0.056 (0.068)\tData 0.012 (0.013)\tLoss 0.4169 (0.2796)\tPrec 90.625% (90.548%)\n",
      "Epoch: [20][200/782]\tTime 0.058 (0.068)\tData 0.012 (0.013)\tLoss 0.3933 (0.2739)\tPrec 85.938% (90.493%)\n",
      "Epoch: [20][300/782]\tTime 0.067 (0.068)\tData 0.015 (0.013)\tLoss 0.3193 (0.2734)\tPrec 89.062% (90.480%)\n",
      "Epoch: [20][400/782]\tTime 0.045 (0.068)\tData 0.012 (0.013)\tLoss 0.3302 (0.2761)\tPrec 87.500% (90.387%)\n",
      "Epoch: [20][500/782]\tTime 0.056 (0.068)\tData 0.012 (0.013)\tLoss 0.3016 (0.2791)\tPrec 87.500% (90.248%)\n",
      "Epoch: [20][600/782]\tTime 0.067 (0.068)\tData 0.012 (0.013)\tLoss 0.3873 (0.2785)\tPrec 85.938% (90.271%)\n",
      "Epoch: [20][700/782]\tTime 0.064 (0.068)\tData 0.018 (0.013)\tLoss 0.3185 (0.2788)\tPrec 85.938% (90.262%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.043 (0.043)\tLoss 0.4173 (0.4173)\tPrec 84.375% (84.375%)\n",
      "Test: [100/157]\tTime 0.031 (0.032)\tLoss 0.4313 (0.4032)\tPrec 92.188% (86.726%)\n",
      " * Prec 86.850% \n",
      "best acc: 86.850000\n",
      "Epoch: [21][0/782]\tTime 0.055 (0.055)\tData 0.017 (0.017)\tLoss 0.1782 (0.1782)\tPrec 93.750% (93.750%)\n",
      "Epoch: [21][100/782]\tTime 0.068 (0.067)\tData 0.012 (0.013)\tLoss 0.1685 (0.2481)\tPrec 92.188% (90.950%)\n",
      "Epoch: [21][200/782]\tTime 0.066 (0.067)\tData 0.012 (0.013)\tLoss 0.4227 (0.2535)\tPrec 89.062% (90.928%)\n",
      "Epoch: [21][300/782]\tTime 0.067 (0.067)\tData 0.012 (0.013)\tLoss 0.2739 (0.2546)\tPrec 90.625% (90.988%)\n",
      "Epoch: [21][400/782]\tTime 0.068 (0.067)\tData 0.012 (0.013)\tLoss 0.3166 (0.2631)\tPrec 85.938% (90.707%)\n",
      "Epoch: [21][500/782]\tTime 0.055 (0.067)\tData 0.012 (0.013)\tLoss 0.2322 (0.2643)\tPrec 90.625% (90.672%)\n",
      "Epoch: [21][600/782]\tTime 0.078 (0.067)\tData 0.025 (0.013)\tLoss 0.2672 (0.2662)\tPrec 89.062% (90.674%)\n",
      "Epoch: [21][700/782]\tTime 0.066 (0.067)\tData 0.012 (0.013)\tLoss 0.3101 (0.2693)\tPrec 90.625% (90.589%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.044 (0.044)\tLoss 0.4102 (0.4102)\tPrec 84.375% (84.375%)\n",
      "Test: [100/157]\tTime 0.033 (0.032)\tLoss 0.4511 (0.4094)\tPrec 82.812% (86.510%)\n",
      " * Prec 86.710% \n",
      "best acc: 86.850000\n",
      "Epoch: [22][0/782]\tTime 0.049 (0.049)\tData 0.015 (0.015)\tLoss 0.2869 (0.2869)\tPrec 89.062% (89.062%)\n",
      "Epoch: [22][100/782]\tTime 0.066 (0.067)\tData 0.013 (0.013)\tLoss 0.2792 (0.2661)\tPrec 90.625% (90.470%)\n",
      "Epoch: [22][200/782]\tTime 0.067 (0.067)\tData 0.012 (0.013)\tLoss 0.1609 (0.2692)\tPrec 95.312% (90.470%)\n",
      "Epoch: [22][300/782]\tTime 0.067 (0.067)\tData 0.024 (0.013)\tLoss 0.2983 (0.2695)\tPrec 90.625% (90.485%)\n",
      "Epoch: [22][400/782]\tTime 0.067 (0.067)\tData 0.012 (0.013)\tLoss 0.1951 (0.2722)\tPrec 93.750% (90.481%)\n",
      "Epoch: [22][500/782]\tTime 0.069 (0.067)\tData 0.012 (0.013)\tLoss 0.1848 (0.2711)\tPrec 93.750% (90.500%)\n",
      "Epoch: [22][600/782]\tTime 0.070 (0.068)\tData 0.013 (0.013)\tLoss 0.4442 (0.2689)\tPrec 82.812% (90.550%)\n",
      "Epoch: [22][700/782]\tTime 0.066 (0.068)\tData 0.012 (0.013)\tLoss 0.2630 (0.2692)\tPrec 89.062% (90.558%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.041 (0.041)\tLoss 0.4564 (0.4564)\tPrec 82.812% (82.812%)\n",
      "Test: [100/157]\tTime 0.031 (0.031)\tLoss 0.4094 (0.4502)\tPrec 84.375% (85.783%)\n",
      " * Prec 85.850% \n",
      "best acc: 86.850000\n",
      "Epoch: [23][0/782]\tTime 0.053 (0.053)\tData 0.019 (0.019)\tLoss 0.2583 (0.2583)\tPrec 92.188% (92.188%)\n",
      "Epoch: [23][100/782]\tTime 0.067 (0.067)\tData 0.012 (0.013)\tLoss 0.1079 (0.2622)\tPrec 96.875% (90.718%)\n",
      "Epoch: [23][200/782]\tTime 0.066 (0.067)\tData 0.012 (0.013)\tLoss 0.2981 (0.2625)\tPrec 90.625% (90.858%)\n",
      "Epoch: [23][300/782]\tTime 0.058 (0.067)\tData 0.013 (0.013)\tLoss 0.1800 (0.2632)\tPrec 93.750% (90.703%)\n",
      "Epoch: [23][400/782]\tTime 0.067 (0.067)\tData 0.012 (0.013)\tLoss 0.2169 (0.2668)\tPrec 93.750% (90.664%)\n",
      "Epoch: [23][500/782]\tTime 0.068 (0.067)\tData 0.012 (0.013)\tLoss 0.2833 (0.2673)\tPrec 92.188% (90.659%)\n",
      "Epoch: [23][600/782]\tTime 0.066 (0.067)\tData 0.012 (0.013)\tLoss 0.2908 (0.2687)\tPrec 93.750% (90.648%)\n",
      "Epoch: [23][700/782]\tTime 0.055 (0.067)\tData 0.012 (0.013)\tLoss 0.1923 (0.2692)\tPrec 93.750% (90.618%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.043 (0.043)\tLoss 0.4530 (0.4530)\tPrec 85.938% (85.938%)\n",
      "Test: [100/157]\tTime 0.031 (0.032)\tLoss 0.5335 (0.4673)\tPrec 81.250% (85.644%)\n",
      " * Prec 85.810% \n",
      "best acc: 86.850000\n",
      "Epoch: [24][0/782]\tTime 0.051 (0.051)\tData 0.018 (0.018)\tLoss 0.2868 (0.2868)\tPrec 89.062% (89.062%)\n",
      "Epoch: [24][100/782]\tTime 0.079 (0.067)\tData 0.012 (0.014)\tLoss 0.1946 (0.2621)\tPrec 90.625% (91.012%)\n",
      "Epoch: [24][200/782]\tTime 0.067 (0.067)\tData 0.012 (0.014)\tLoss 0.1420 (0.2620)\tPrec 95.312% (90.889%)\n",
      "Epoch: [24][300/782]\tTime 0.059 (0.067)\tData 0.013 (0.013)\tLoss 0.1239 (0.2575)\tPrec 95.312% (90.983%)\n",
      "Epoch: [24][400/782]\tTime 0.067 (0.067)\tData 0.012 (0.013)\tLoss 0.1815 (0.2556)\tPrec 95.312% (91.116%)\n",
      "Epoch: [24][500/782]\tTime 0.067 (0.067)\tData 0.012 (0.013)\tLoss 0.1181 (0.2578)\tPrec 95.312% (91.037%)\n",
      "Epoch: [24][600/782]\tTime 0.067 (0.067)\tData 0.012 (0.013)\tLoss 0.1685 (0.2604)\tPrec 93.750% (90.929%)\n",
      "Epoch: [24][700/782]\tTime 0.066 (0.067)\tData 0.012 (0.013)\tLoss 0.1914 (0.2625)\tPrec 95.312% (90.843%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.032 (0.032)\tLoss 0.2949 (0.2949)\tPrec 85.938% (85.938%)\n",
      "Test: [100/157]\tTime 0.031 (0.031)\tLoss 0.4150 (0.4440)\tPrec 85.938% (85.938%)\n",
      " * Prec 86.280% \n",
      "best acc: 86.850000\n",
      "Epoch: [25][0/782]\tTime 0.068 (0.068)\tData 0.021 (0.021)\tLoss 0.3560 (0.3560)\tPrec 89.062% (89.062%)\n",
      "Epoch: [25][100/782]\tTime 0.066 (0.067)\tData 0.012 (0.013)\tLoss 0.2187 (0.2508)\tPrec 90.625% (90.965%)\n",
      "Epoch: [25][200/782]\tTime 0.066 (0.067)\tData 0.012 (0.013)\tLoss 0.2616 (0.2477)\tPrec 90.625% (90.990%)\n",
      "Epoch: [25][300/782]\tTime 0.081 (0.067)\tData 0.025 (0.013)\tLoss 0.3649 (0.2544)\tPrec 89.062% (90.827%)\n",
      "Epoch: [25][400/782]\tTime 0.067 (0.067)\tData 0.013 (0.013)\tLoss 0.2409 (0.2577)\tPrec 92.188% (90.816%)\n",
      "Epoch: [25][500/782]\tTime 0.081 (0.067)\tData 0.012 (0.013)\tLoss 0.2947 (0.2607)\tPrec 89.062% (90.744%)\n",
      "Epoch: [25][600/782]\tTime 0.067 (0.067)\tData 0.013 (0.013)\tLoss 0.2170 (0.2610)\tPrec 90.625% (90.797%)\n",
      "Epoch: [25][700/782]\tTime 0.067 (0.067)\tData 0.012 (0.013)\tLoss 0.3593 (0.2608)\tPrec 87.500% (90.803%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.044 (0.044)\tLoss 0.4701 (0.4701)\tPrec 84.375% (84.375%)\n",
      "Test: [100/157]\tTime 0.031 (0.031)\tLoss 0.4854 (0.4484)\tPrec 89.062% (85.876%)\n",
      " * Prec 85.440% \n",
      "best acc: 86.850000\n",
      "Epoch: [26][0/782]\tTime 0.053 (0.053)\tData 0.017 (0.017)\tLoss 0.1517 (0.1517)\tPrec 95.312% (95.312%)\n",
      "Epoch: [26][100/782]\tTime 0.066 (0.067)\tData 0.012 (0.014)\tLoss 0.2558 (0.2493)\tPrec 92.188% (91.429%)\n",
      "Epoch: [26][200/782]\tTime 0.057 (0.067)\tData 0.013 (0.013)\tLoss 0.1324 (0.2465)\tPrec 96.875% (91.535%)\n",
      "Epoch: [26][300/782]\tTime 0.066 (0.067)\tData 0.012 (0.013)\tLoss 0.4317 (0.2521)\tPrec 84.375% (91.315%)\n",
      "Epoch: [26][400/782]\tTime 0.056 (0.067)\tData 0.012 (0.013)\tLoss 0.1870 (0.2558)\tPrec 95.312% (91.096%)\n",
      "Epoch: [26][500/782]\tTime 0.046 (0.067)\tData 0.012 (0.013)\tLoss 0.2583 (0.2600)\tPrec 93.750% (90.971%)\n",
      "Epoch: [26][600/782]\tTime 0.053 (0.067)\tData 0.012 (0.013)\tLoss 0.5544 (0.2612)\tPrec 81.250% (90.906%)\n",
      "Epoch: [26][700/782]\tTime 0.066 (0.067)\tData 0.026 (0.013)\tLoss 0.2896 (0.2628)\tPrec 89.062% (90.810%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.034 (0.034)\tLoss 0.4580 (0.4580)\tPrec 85.938% (85.938%)\n",
      "Test: [100/157]\tTime 0.031 (0.031)\tLoss 0.4338 (0.4093)\tPrec 85.938% (87.268%)\n",
      " * Prec 87.140% \n",
      "best acc: 87.140000\n",
      "Epoch: [27][0/782]\tTime 0.084 (0.084)\tData 0.021 (0.021)\tLoss 0.3004 (0.3004)\tPrec 87.500% (87.500%)\n",
      "Epoch: [27][100/782]\tTime 0.066 (0.067)\tData 0.027 (0.016)\tLoss 0.3959 (0.2508)\tPrec 87.500% (91.337%)\n",
      "Epoch: [27][200/782]\tTime 0.082 (0.067)\tData 0.012 (0.016)\tLoss 0.3243 (0.2548)\tPrec 89.062% (91.200%)\n",
      "Epoch: [27][300/782]\tTime 0.067 (0.067)\tData 0.017 (0.016)\tLoss 0.3368 (0.2555)\tPrec 87.500% (91.191%)\n",
      "Epoch: [27][400/782]\tTime 0.067 (0.067)\tData 0.012 (0.016)\tLoss 0.1498 (0.2570)\tPrec 93.750% (91.046%)\n",
      "Epoch: [27][500/782]\tTime 0.067 (0.067)\tData 0.012 (0.016)\tLoss 0.2586 (0.2599)\tPrec 93.750% (90.893%)\n",
      "Epoch: [27][600/782]\tTime 0.066 (0.067)\tData 0.024 (0.016)\tLoss 0.2907 (0.2580)\tPrec 89.062% (91.010%)\n",
      "Epoch: [27][700/782]\tTime 0.066 (0.067)\tData 0.012 (0.016)\tLoss 0.2533 (0.2592)\tPrec 93.750% (90.975%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.042 (0.042)\tLoss 0.4252 (0.4252)\tPrec 85.938% (85.938%)\n",
      "Test: [100/157]\tTime 0.032 (0.031)\tLoss 0.6346 (0.4605)\tPrec 79.688% (85.613%)\n",
      " * Prec 85.360% \n",
      "best acc: 87.140000\n",
      "Epoch: [28][0/782]\tTime 0.061 (0.061)\tData 0.017 (0.017)\tLoss 0.3024 (0.3024)\tPrec 89.062% (89.062%)\n",
      "Epoch: [28][100/782]\tTime 0.066 (0.067)\tData 0.013 (0.013)\tLoss 0.4033 (0.2549)\tPrec 84.375% (90.873%)\n",
      "Epoch: [28][200/782]\tTime 0.058 (0.067)\tData 0.012 (0.014)\tLoss 0.1031 (0.2570)\tPrec 98.438% (90.882%)\n",
      "Epoch: [28][300/782]\tTime 0.054 (0.067)\tData 0.012 (0.014)\tLoss 0.2233 (0.2584)\tPrec 95.312% (90.812%)\n",
      "Epoch: [28][400/782]\tTime 0.055 (0.067)\tData 0.012 (0.014)\tLoss 0.3946 (0.2576)\tPrec 82.812% (90.941%)\n",
      "Epoch: [28][500/782]\tTime 0.066 (0.067)\tData 0.012 (0.014)\tLoss 0.2711 (0.2580)\tPrec 92.188% (90.952%)\n",
      "Epoch: [28][600/782]\tTime 0.054 (0.067)\tData 0.013 (0.013)\tLoss 0.2103 (0.2579)\tPrec 89.062% (90.906%)\n",
      "Epoch: [28][700/782]\tTime 0.066 (0.067)\tData 0.013 (0.013)\tLoss 0.2559 (0.2569)\tPrec 92.188% (90.937%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.044 (0.044)\tLoss 0.5125 (0.5125)\tPrec 85.938% (85.938%)\n",
      "Test: [100/157]\tTime 0.032 (0.032)\tLoss 0.5209 (0.4319)\tPrec 85.938% (86.773%)\n",
      " * Prec 86.810% \n",
      "best acc: 87.140000\n",
      "Epoch: [29][0/782]\tTime 0.053 (0.053)\tData 0.018 (0.018)\tLoss 0.1871 (0.1871)\tPrec 92.188% (92.188%)\n",
      "Epoch: [29][100/782]\tTime 0.068 (0.069)\tData 0.012 (0.013)\tLoss 0.0861 (0.2455)\tPrec 98.438% (91.058%)\n",
      "Epoch: [29][200/782]\tTime 0.067 (0.068)\tData 0.012 (0.013)\tLoss 0.2522 (0.2369)\tPrec 90.625% (91.527%)\n",
      "Epoch: [29][300/782]\tTime 0.068 (0.068)\tData 0.012 (0.013)\tLoss 0.3396 (0.2400)\tPrec 87.500% (91.533%)\n",
      "Epoch: [29][400/782]\tTime 0.073 (0.068)\tData 0.012 (0.013)\tLoss 0.2374 (0.2432)\tPrec 93.750% (91.525%)\n",
      "Epoch: [29][500/782]\tTime 0.068 (0.068)\tData 0.013 (0.013)\tLoss 0.3035 (0.2447)\tPrec 87.500% (91.367%)\n",
      "Epoch: [29][600/782]\tTime 0.057 (0.068)\tData 0.013 (0.013)\tLoss 0.3614 (0.2455)\tPrec 93.750% (91.306%)\n",
      "Epoch: [29][700/782]\tTime 0.068 (0.068)\tData 0.012 (0.013)\tLoss 0.2193 (0.2461)\tPrec 92.188% (91.300%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.035 (0.035)\tLoss 0.4936 (0.4936)\tPrec 84.375% (84.375%)\n",
      "Test: [100/157]\tTime 0.031 (0.032)\tLoss 0.7299 (0.4557)\tPrec 78.125% (85.953%)\n",
      " * Prec 85.830% \n",
      "best acc: 87.140000\n",
      "Epoch: [30][0/782]\tTime 0.067 (0.067)\tData 0.016 (0.016)\tLoss 0.2071 (0.2071)\tPrec 93.750% (93.750%)\n",
      "Epoch: [30][100/782]\tTime 0.068 (0.068)\tData 0.012 (0.013)\tLoss 0.2632 (0.2313)\tPrec 90.625% (92.079%)\n",
      "Epoch: [30][200/782]\tTime 0.068 (0.069)\tData 0.012 (0.013)\tLoss 0.4393 (0.2383)\tPrec 85.938% (91.659%)\n",
      "Epoch: [30][300/782]\tTime 0.068 (0.068)\tData 0.013 (0.013)\tLoss 0.2124 (0.2411)\tPrec 95.312% (91.518%)\n",
      "Epoch: [30][400/782]\tTime 0.078 (0.069)\tData 0.012 (0.013)\tLoss 0.2700 (0.2417)\tPrec 90.625% (91.506%)\n",
      "Epoch: [30][500/782]\tTime 0.067 (0.069)\tData 0.013 (0.013)\tLoss 0.2567 (0.2418)\tPrec 90.625% (91.389%)\n",
      "Epoch: [30][600/782]\tTime 0.070 (0.069)\tData 0.012 (0.013)\tLoss 0.1785 (0.2429)\tPrec 95.312% (91.363%)\n",
      "Epoch: [30][700/782]\tTime 0.067 (0.069)\tData 0.012 (0.013)\tLoss 0.2251 (0.2450)\tPrec 93.750% (91.329%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.044 (0.044)\tLoss 0.3133 (0.3133)\tPrec 87.500% (87.500%)\n",
      "Test: [100/157]\tTime 0.032 (0.032)\tLoss 0.3746 (0.4367)\tPrec 84.375% (86.463%)\n",
      " * Prec 86.480% \n",
      "best acc: 87.140000\n",
      "Epoch: [31][0/782]\tTime 0.051 (0.051)\tData 0.017 (0.017)\tLoss 0.2813 (0.2813)\tPrec 87.500% (87.500%)\n",
      "Epoch: [31][100/782]\tTime 0.069 (0.068)\tData 0.012 (0.014)\tLoss 0.2273 (0.2357)\tPrec 89.062% (91.708%)\n",
      "Epoch: [31][200/782]\tTime 0.078 (0.068)\tData 0.012 (0.014)\tLoss 0.3110 (0.2379)\tPrec 89.062% (91.737%)\n",
      "Epoch: [31][300/782]\tTime 0.082 (0.068)\tData 0.012 (0.014)\tLoss 0.2829 (0.2413)\tPrec 89.062% (91.482%)\n",
      "Epoch: [31][400/782]\tTime 0.069 (0.068)\tData 0.013 (0.013)\tLoss 0.0766 (0.2408)\tPrec 98.438% (91.615%)\n",
      "Epoch: [31][500/782]\tTime 0.055 (0.069)\tData 0.012 (0.014)\tLoss 0.2914 (0.2442)\tPrec 92.188% (91.498%)\n",
      "Epoch: [31][600/782]\tTime 0.066 (0.069)\tData 0.012 (0.014)\tLoss 0.1987 (0.2448)\tPrec 92.188% (91.462%)\n",
      "Epoch: [31][700/782]\tTime 0.066 (0.069)\tData 0.013 (0.014)\tLoss 0.1873 (0.2466)\tPrec 93.750% (91.401%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.032 (0.032)\tLoss 0.4569 (0.4569)\tPrec 84.375% (84.375%)\n",
      "Test: [100/157]\tTime 0.031 (0.033)\tLoss 0.3920 (0.4177)\tPrec 81.250% (86.881%)\n",
      " * Prec 86.960% \n",
      "best acc: 87.140000\n",
      "Epoch: [32][0/782]\tTime 0.058 (0.058)\tData 0.025 (0.025)\tLoss 0.1147 (0.1147)\tPrec 96.875% (96.875%)\n",
      "Epoch: [32][100/782]\tTime 0.066 (0.072)\tData 0.014 (0.013)\tLoss 0.2268 (0.2163)\tPrec 89.062% (92.234%)\n",
      "Epoch: [32][200/782]\tTime 0.080 (0.070)\tData 0.012 (0.013)\tLoss 0.4403 (0.2214)\tPrec 85.938% (92.203%)\n",
      "Epoch: [32][300/782]\tTime 0.070 (0.070)\tData 0.013 (0.013)\tLoss 0.2759 (0.2297)\tPrec 89.062% (92.021%)\n",
      "Epoch: [32][400/782]\tTime 0.066 (0.070)\tData 0.012 (0.013)\tLoss 0.2390 (0.2338)\tPrec 93.750% (91.856%)\n",
      "Epoch: [32][500/782]\tTime 0.066 (0.071)\tData 0.013 (0.013)\tLoss 0.3242 (0.2347)\tPrec 90.625% (91.798%)\n",
      "Epoch: [32][600/782]\tTime 0.069 (0.070)\tData 0.014 (0.014)\tLoss 0.1459 (0.2394)\tPrec 93.750% (91.644%)\n",
      "Epoch: [32][700/782]\tTime 0.068 (0.070)\tData 0.013 (0.014)\tLoss 0.1723 (0.2411)\tPrec 92.188% (91.619%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.043 (0.043)\tLoss 0.4651 (0.4651)\tPrec 85.938% (85.938%)\n",
      "Test: [100/157]\tTime 0.031 (0.032)\tLoss 0.6440 (0.4269)\tPrec 81.250% (87.515%)\n",
      " * Prec 87.360% \n",
      "best acc: 87.360000\n",
      "Epoch: [33][0/782]\tTime 0.053 (0.053)\tData 0.019 (0.019)\tLoss 0.2934 (0.2934)\tPrec 90.625% (90.625%)\n",
      "Epoch: [33][100/782]\tTime 0.069 (0.070)\tData 0.012 (0.014)\tLoss 0.2498 (0.2310)\tPrec 93.750% (91.723%)\n",
      "Epoch: [33][200/782]\tTime 0.056 (0.069)\tData 0.013 (0.013)\tLoss 0.2289 (0.2407)\tPrec 90.625% (91.550%)\n",
      "Epoch: [33][300/782]\tTime 0.083 (0.069)\tData 0.012 (0.013)\tLoss 0.2750 (0.2420)\tPrec 87.500% (91.544%)\n",
      "Epoch: [33][400/782]\tTime 0.066 (0.068)\tData 0.012 (0.013)\tLoss 0.1191 (0.2421)\tPrec 95.312% (91.490%)\n",
      "Epoch: [33][500/782]\tTime 0.067 (0.068)\tData 0.012 (0.013)\tLoss 0.2080 (0.2421)\tPrec 92.188% (91.564%)\n",
      "Epoch: [33][600/782]\tTime 0.067 (0.068)\tData 0.012 (0.013)\tLoss 0.2811 (0.2447)\tPrec 89.062% (91.415%)\n",
      "Epoch: [33][700/782]\tTime 0.066 (0.068)\tData 0.012 (0.013)\tLoss 0.2068 (0.2432)\tPrec 93.750% (91.463%)\n",
      "Validation starts\n",
      "Test: [0/157]\tTime 0.043 (0.043)\tLoss 0.3853 (0.3853)\tPrec 85.938% (85.938%)\n",
      "Test: [100/157]\tTime 0.032 (0.032)\tLoss 0.5060 (0.4132)\tPrec 81.250% (86.850%)\n",
      " * Prec 87.040% \n",
      "best acc: 87.360000\n",
      "Epoch: [34][0/782]\tTime 0.086 (0.086)\tData 0.033 (0.033)\tLoss 0.1795 (0.1795)\tPrec 95.312% (95.312%)\n",
      "Epoch: [34][100/782]\tTime 0.066 (0.067)\tData 0.012 (0.014)\tLoss 0.2584 (0.2361)\tPrec 92.188% (91.383%)\n",
      "Epoch: [34][200/782]\tTime 0.058 (0.067)\tData 0.013 (0.013)\tLoss 0.2915 (0.2325)\tPrec 85.938% (91.604%)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 211\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, fdir, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, epochs):\n\u001b[1;32m    209\u001b[0m     adjust_learning_rate(optimizer, epoch)\n\u001b[0;32m--> 211\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# evaluate on test set\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation starts\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 80\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(trainloader, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (\u001b[38;5;28minput\u001b[39m, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trainloader):\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m# measure data loading time\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     data_time\u001b[38;5;241m.\u001b[39mupdate(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m end)\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28minput\u001b[39m, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, target\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# compute output\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 3e-3\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "train_model(model, fdir, criterion, optimizer, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning 50 ic-slices out of 64 ic-slices (78.1% pruned)\n",
      "Pruning 50 ic-slices out of 64 ic-slices (78.1% pruned)\n",
      "Pruning 100 ic-slices out of 128 ic-slices (78.1% pruned)\n",
      "Pruning 100 ic-slices out of 128 ic-slices (78.1% pruned)\n",
      "Pruning 200 ic-slices out of 256 ic-slices (78.1% pruned)\n",
      "Pruning 200 ic-slices out of 256 ic-slices (78.1% pruned)\n",
      "Pruning 200 ic-slices out of 256 ic-slices (78.1% pruned)\n",
      "Pruning 399 ic-slices out of 512 ic-slices (77.9% pruned)\n",
      "Pruning 399 ic-slices out of 512 ic-slices (77.9% pruned)\n",
      "Pruning 399 ic-slices out of 512 ic-slices (77.9% pruned)\n",
      "Pruning 399 ic-slices out of 512 ic-slices (77.9% pruned)\n",
      "Pruning 399 ic-slices out of 512 ic-slices (77.9% pruned)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63507/3884249786.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 8736/10000 (87%)\n",
      "\n",
      "layer 3 sparsity: 0.781\n",
      "layer 7 sparsity: 0.781\n",
      "layer 10 sparsity: 0.773\n",
      "layer 14 sparsity: 0.773\n",
      "layer 17 sparsity: 0.773\n",
      "layer 20 sparsity: 0.773\n",
      "layer 24 sparsity: 0.773\n",
      "layer 27 sparsity: 0.777\n",
      "layer 30 sparsity: 0.777\n",
      "layer 34 sparsity: 0.777\n",
      "layer 37 sparsity: 0.777\n",
      "layer 40 sparsity: 0.777\n"
     ]
    }
   ],
   "source": [
    "model = VGG16()\n",
    "os_prune_vgg16(model, 0.78)\n",
    "quantize_pruned(model)\n",
    "\n",
    "PATH = f\"{fdir}/model_best.pth.tar\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.cuda()\n",
    "\n",
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "    def __call__(self, module, module_in):\n",
    "        self.outputs.append(module_in)\n",
    "    def clear(self):\n",
    "        self.outputs = []\n",
    "save_output = SaveOutput()\n",
    "model.features[40].register_forward_pre_hook(save_output)\n",
    "\n",
    "val_model(model)\n",
    "\n",
    "print_sparsity(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000, -1.4775, -1.4775],\n",
      "         [ 0.7387,  0.7387,  0.0000],\n",
      "         [ 0.0000,  0.7387,  0.0000]],\n",
      "\n",
      "        [[-0.0000,  0.0000,  0.7387],\n",
      "         [ 0.0000,  0.7387,  1.4775],\n",
      "         [ 0.7387,  0.7387,  0.7387]]], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "l = model.features[40]\n",
    "a = [i for i in range(l.weight_mask.size(1)) if l.weight_mask[0,i].sum() > 0]\n",
    "print(l.weight_q[0,a[:2],:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0000, 0.0807],\n",
      "          [0.1303, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.5222, 0.3067],\n",
      "          [0.0775, 0.9398]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.0000]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(save_output.outputs[0][0][:2,:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight int: \n",
      "tensor([[ 0.0000, -2.0000, -2.0000],\n",
      "        [ 1.0000,  1.0000,  0.0000],\n",
      "        [ 0.0000,  1.0000,  0.0000]], device='cuda:0')\n",
      "Act int: \n",
      "tensor([[[[0., 0.],\n",
      "          [0., 0.]],\n",
      "\n",
      "         [[0., 0.],\n",
      "          [0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1.],\n",
      "          [0., 2.]],\n",
      "\n",
      "         [[0., 0.],\n",
      "          [0., 0.]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(f'Weight int: \\n{(model.features[40].weight_q.data / (model.features[40].weight_quant.wgt_alpha.data.item()/(2**(4-1)-1)))[0,a[0]]}')\n",
    "x = save_output.outputs[0][0]\n",
    "x_alpha = model.features[40].act_alpha.data.item()\n",
    "x_delta = x_alpha / (2**(4)-1)\n",
    "act_q = act_quantization(4)\n",
    "x_q = act_q(x, x_alpha)\n",
    "print(f'Act int: \\n{(x_q/x_delta)[:2,:2]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-forge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
